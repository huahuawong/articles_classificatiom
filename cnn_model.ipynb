{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dcba625",
   "metadata": {},
   "source": [
    "## 1. Getting all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd8dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D, Embedding, Input\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "import itertools\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7784b18",
   "metadata": {},
   "source": [
    "## 2. Loading data\n",
    "\n",
    "Read the data that was stored in the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd48dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\\arxiv-metadata-oai-snapshot.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>journal-ref</th>\n",
       "      <th>doi</th>\n",
       "      <th>report-no</th>\n",
       "      <th>categories</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>versions</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0704.0001</td>\n",
       "      <td>Pavel Nadolsky</td>\n",
       "      <td>C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...</td>\n",
       "      <td>Calculation of prompt diphoton production cros...</td>\n",
       "      <td>37 pages, 15 figures; published version</td>\n",
       "      <td>Phys.Rev.D76:013009,2007</td>\n",
       "      <td>10.1103/PhysRevD.76.013009</td>\n",
       "      <td>ANL-HEP-PR-07-12</td>\n",
       "      <td>hep-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>A fully differential calculation in perturba...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>[[Bal√°zs, C., ], [Berger, E. L., ], [Nadolsky,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0704.0002</td>\n",
       "      <td>Louis Theran</td>\n",
       "      <td>Ileana Streinu and Louis Theran</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>To appear in Graphs and Combinatorics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>http://arxiv.org/licenses/nonexclusive-distrib...</td>\n",
       "      <td>We describe a new algorithm, the $(k,\\ell)$-...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>[[Streinu, Ileana, ], [Theran, Louis, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0704.0003</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>Hongjun Pan</td>\n",
       "      <td>The evolution of the Earth-Moon system based o...</td>\n",
       "      <td>23 pages, 3 figures</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>None</td>\n",
       "      <td>The evolution of Earth-Moon system is descri...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sun, 1 Apr 2007...</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>[[Pan, Hongjun, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0704.0004</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>David Callan</td>\n",
       "      <td>A determinant of Stirling cycle numbers counts...</td>\n",
       "      <td>11 pages</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CO</td>\n",
       "      <td>None</td>\n",
       "      <td>We show that a determinant of Stirling cycle...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Sat, 31 Mar 200...</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>[[Callan, David, ]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0704.0005</td>\n",
       "      <td>Alberto Torchinsky</td>\n",
       "      <td>Wael Abu-Shammala and Alberto Torchinsky</td>\n",
       "      <td>From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...</td>\n",
       "      <td>None</td>\n",
       "      <td>Illinois J. Math. 52 (2008) no.2, 681-689</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>None</td>\n",
       "      <td>In this paper we show how to compute the $\\L...</td>\n",
       "      <td>[{'version': 'v1', 'created': 'Mon, 2 Apr 2007...</td>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>[[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id           submitter  \\\n",
       "0  0704.0001      Pavel Nadolsky   \n",
       "1  0704.0002        Louis Theran   \n",
       "2  0704.0003         Hongjun Pan   \n",
       "3  0704.0004        David Callan   \n",
       "4  0704.0005  Alberto Torchinsky   \n",
       "\n",
       "                                             authors  \\\n",
       "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
       "1                    Ileana Streinu and Louis Theran   \n",
       "2                                        Hongjun Pan   \n",
       "3                                       David Callan   \n",
       "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
       "\n",
       "                                               title  \\\n",
       "0  Calculation of prompt diphoton production cros...   \n",
       "1           Sparsity-certifying Graph Decompositions   \n",
       "2  The evolution of the Earth-Moon system based o...   \n",
       "3  A determinant of Stirling cycle numbers counts...   \n",
       "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
       "\n",
       "                                  comments  \\\n",
       "0  37 pages, 15 figures; published version   \n",
       "1    To appear in Graphs and Combinatorics   \n",
       "2                      23 pages, 3 figures   \n",
       "3                                 11 pages   \n",
       "4                                     None   \n",
       "\n",
       "                                 journal-ref                         doi  \\\n",
       "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
       "1                                       None                        None   \n",
       "2                                       None                        None   \n",
       "3                                       None                        None   \n",
       "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
       "\n",
       "          report-no       categories  \\\n",
       "0  ANL-HEP-PR-07-12           hep-ph   \n",
       "1              None    math.CO cs.CG   \n",
       "2              None   physics.gen-ph   \n",
       "3              None          math.CO   \n",
       "4              None  math.CA math.FA   \n",
       "\n",
       "                                             license  \\\n",
       "0                                               None   \n",
       "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                            abstract  \\\n",
       "0    A fully differential calculation in perturba...   \n",
       "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
       "2    The evolution of Earth-Moon system is descri...   \n",
       "3    We show that a determinant of Stirling cycle...   \n",
       "4    In this paper we show how to compute the $\\L...   \n",
       "\n",
       "                                            versions update_date  \\\n",
       "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2008-11-26   \n",
       "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2008-12-13   \n",
       "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  2008-01-13   \n",
       "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2007-05-23   \n",
       "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2013-10-15   \n",
       "\n",
       "                                      authors_parsed  \n",
       "0  [[Bal√°zs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
       "1           [[Streinu, Ileana, ], [Theran, Louis, ]]  \n",
       "2                                 [[Pan, Hongjun, ]]  \n",
       "3                                [[Callan, David, ]]  \n",
       "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('./data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "def load_file():\n",
    "    with open('./data/arxiv-metadata-oai-snapshot.json') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "\n",
    "metadata = load_file()\n",
    "\n",
    "subset = itertools.islice(metadata, 100000)\n",
    "\n",
    "df = pd.DataFrame(subset)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c365ff8",
   "metadata": {},
   "source": [
    "Similar to what we did for the \"bow_model.ipynb\" notebook, we are only looking into the 6 major categories for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34912c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df[df['categories'].isin(['hep-ph', 'quant-ph', 'astro-ph', 'hep-th', 'gr-qc', 'cond-mat.mtrl-sci'])] # high energy physics/quantum physics\n",
    "df_cat['categories'].value_counts()\n",
    "\n",
    "df_cat = pd.get_dummies(df_cat, columns=['categories'])\n",
    "\n",
    "X = df_cat['abstract']\n",
    "\n",
    "# Dependent Variables\n",
    "y = df_cat[['categories_astro-ph', 'categories_gr-qc',\n",
    "            'categories_hep-ph', 'categories_hep-th', 'categories_quant-ph', 'categories_cond-mat.mtrl-sci']]\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(X, y, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbaf23",
   "metadata": {},
   "source": [
    "## 3. Text preprocessing\n",
    "\n",
    "1. We first initialize the tokenizer to split the words into lists of tokens. The num_words is set to 20000, which is the maximum number to keep.\n",
    "2. Using 'fit_on_texts' function to update the internal vocabulary based on the list of texts. \n",
    "3. Calling the 'texts_to_sequences' function which transforms each text in texts to a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da50332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word Index: 46580\n",
      "First 5 elements in the word_index dictionary: {'the': 1, 'of': 2, 'and': 3, 'a': 4, 'in': 5}\n",
      "First comment text in training set:\n",
      " [7, 885, 72, 2792, 183, 1, 1338, 1921, 4509, 602, 13, 3816, 916, 39, 408, 1, 1013, 541, 1339, 496, 4195, 1932, 950, 39, 9, 1, 2215, 123, 3722, 91, 2, 1542, 176, 974, 4, 3, 64, 135, 17, 85, 17, 2617, 1719, 7, 682, 1013, 100, 255, 1104, 1961, 404, 2, 4346, 649, 3, 43, 25, 55, 341, 179, 180, 39, 16, 1, 7391, 219, 53, 2, 1, 1463, 1104, 653, 2829, 6, 1714, 5, 30, 2670, 7, 885, 1, 6069, 2, 1, 3722, 1815, 4, 64, 3, 4, 22, 1434, 5, 5022, 916, 3, 496, 4195, 39, 18, 90, 650, 3, 2041, 1, 4594, 3, 2378, 182, 7, 36, 473, 1, 515, 46, 82, 182, 3, 1, 271, 46, 82, 182, 77, 17, 4, 118, 2, 194, 42, 916, 3, 950, 141, 565, 13, 3722]\n"
     ]
    }
   ],
   "source": [
    "# Initializing the class\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "# Updates internal vocabulary based on a list of texts.\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Transforms each text in texts to a sequence of integers.\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Length of word Index:\", len(word_index))\n",
    "print(\"First 5 elements in the word_index dictionary:\", dict(list(word_index.items())[0: 5]) )\n",
    "print(\"First abstract text in training set:\\n\", train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed119380",
   "metadata": {},
   "source": [
    "Now that we have tokenized the comment texts, we need to pad the sentences to make all the sentences of equal length. This is because for DL model inputs, we should have a fixed length of data inputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9895ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded sequence list:\n",
      " (23494, 1000)\n",
      "First abstract text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\n",
      " [  64    3    4   22 1434    5 5022  916    3  496 4195   39   18   90\n",
      "  650    3 2041    1 4594    3 2378  182    7   36  473    1  515   46\n",
      "   82  182    3    1  271   46   82  182   77   17    4  118    2  194\n",
      "   42  916    3  950  141  565   13 3722]\n"
     ]
    }
   ],
   "source": [
    "# Pad tokenized sequences\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"Shape of padded sequence list:\\n\", trainvalid_data.shape)\n",
    "print(\"First abstract text in training set - 0 for padding - only last 50 sequences as the rest are paddings:\\n\", trainvalid_data[0][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607ecd3",
   "metadata": {},
   "source": [
    "## 4. Data modeling\n",
    "\n",
    "CNN 1D model is used to classify the articles. A standard model for document classification is to use an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer. We used 1 embedding layer, 3 sets of Convolution and Pooling layer and 2 sets of Dense layer. These hyperparameters can be fine-tuned to improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49fb39cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 2,823,430\n",
      "Trainable params: 2,823,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Using 1D CNN model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\n",
    "cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\n",
    "cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = \"relu\"))\n",
    "cnn_model.add(GlobalMaxPool1D())\n",
    "cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "cnn_model.add(Dense(units = 6, activation = 'sigmoid'))\n",
    "\n",
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6797f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17620, 1000) (17620, 6) (5874, 1000) (5874, 6)\n",
      "Epoch 1/20\n",
      "138/138 [==============================] - 201s 1s/step - loss: 0.2835 - auc: 0.8698 - accuracy: 0.5763 - val_loss: 0.2156 - val_auc: 0.9269 - val_accuracy: 0.6442\n",
      "Epoch 2/20\n",
      "138/138 [==============================] - 214s 2s/step - loss: 0.1889 - auc: 0.9487 - accuracy: 0.7019 - val_loss: 0.1631 - val_auc: 0.9643 - val_accuracy: 0.7930\n",
      "Epoch 3/20\n",
      "138/138 [==============================] - 210s 2s/step - loss: 0.0948 - auc: 0.9879 - accuracy: 0.8898 - val_loss: 0.0795 - val_auc: 0.9911 - val_accuracy: 0.9067\n",
      "Epoch 4/20\n",
      "138/138 [==============================] - 220s 2s/step - loss: 0.0375 - auc: 0.9977 - accuracy: 0.9591 - val_loss: 0.0846 - val_auc: 0.9893 - val_accuracy: 0.9162\n",
      "Epoch 5/20\n",
      "138/138 [==============================] - 192s 1s/step - loss: 0.0192 - auc: 0.9992 - accuracy: 0.9809 - val_loss: 0.1012 - val_auc: 0.9841 - val_accuracy: 0.9145\n",
      "Epoch 6/20\n",
      "138/138 [==============================] - 188s 1s/step - loss: 0.0100 - auc: 0.9996 - accuracy: 0.9912 - val_loss: 0.1038 - val_auc: 0.9829 - val_accuracy: 0.9198\n",
      "Epoch 7/20\n",
      "138/138 [==============================] - 186s 1s/step - loss: 0.0067 - auc: 0.9998 - accuracy: 0.9939 - val_loss: 0.1399 - val_auc: 0.9776 - val_accuracy: 0.9111\n",
      "Epoch 8/20\n",
      "138/138 [==============================] - 186s 1s/step - loss: 0.0039 - auc: 0.9998 - accuracy: 0.9964 - val_loss: 0.1516 - val_auc: 0.9755 - val_accuracy: 0.9168\n",
      "Epoch 9/20\n",
      "138/138 [==============================] - 186s 1s/step - loss: 0.0028 - auc: 0.9999 - accuracy: 0.9974 - val_loss: 0.1486 - val_auc: 0.9779 - val_accuracy: 0.9193\n",
      "Epoch 10/20\n",
      "138/138 [==============================] - 186s 1s/step - loss: 0.0033 - auc: 0.9998 - accuracy: 0.9968 - val_loss: 0.1517 - val_auc: 0.9771 - val_accuracy: 0.9227\n",
      "Epoch 11/20\n",
      "138/138 [==============================] - 188s 1s/step - loss: 0.0039 - auc: 0.9997 - accuracy: 0.9964 - val_loss: 0.1596 - val_auc: 0.9762 - val_accuracy: 0.9176\n",
      "Epoch 12/20\n",
      "138/138 [==============================] - 189s 1s/step - loss: 0.0073 - auc: 0.9997 - accuracy: 0.9932 - val_loss: 0.1621 - val_auc: 0.9757 - val_accuracy: 0.9139\n",
      "Epoch 13/20\n",
      "138/138 [==============================] - 188s 1s/step - loss: 0.0053 - auc: 0.9996 - accuracy: 0.9952 - val_loss: 0.1583 - val_auc: 0.9749 - val_accuracy: 0.9142\n",
      "Epoch 14/20\n",
      "138/138 [==============================] - 187s 1s/step - loss: 0.0050 - auc: 0.9996 - accuracy: 0.9958 - val_loss: 0.1412 - val_auc: 0.9785 - val_accuracy: 0.9176\n",
      "Epoch 15/20\n",
      "138/138 [==============================] - 186s 1s/step - loss: 0.0025 - auc: 0.9998 - accuracy: 0.9980 - val_loss: 0.1596 - val_auc: 0.9748 - val_accuracy: 0.9152\n",
      "Epoch 16/20\n",
      "138/138 [==============================] - 202s 1s/step - loss: 0.0011 - auc: 0.9999 - accuracy: 0.9990 - val_loss: 0.1602 - val_auc: 0.9743 - val_accuracy: 0.9186\n",
      "Epoch 17/20\n",
      "138/138 [==============================] - 187s 1s/step - loss: 8.2609e-04 - auc: 1.0000 - accuracy: 0.9992 - val_loss: 0.1734 - val_auc: 0.9723 - val_accuracy: 0.9171\n",
      "Epoch 18/20\n",
      "138/138 [==============================] - 184s 1s/step - loss: 9.5793e-04 - auc: 1.0000 - accuracy: 0.9992 - val_loss: 0.1830 - val_auc: 0.9725 - val_accuracy: 0.9183\n",
      "Epoch 19/20\n",
      "138/138 [==============================] - 201s 1s/step - loss: 0.0025 - auc: 0.9999 - accuracy: 0.9976 - val_loss: 0.2196 - val_auc: 0.9686 - val_accuracy: 0.9069\n",
      "Epoch 20/20\n",
      "138/138 [==============================] - 217s 2s/step - loss: 0.0041 - auc: 0.9997 - accuracy: 0.9963 - val_loss: 0.2123 - val_auc: 0.9675 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# Configures the model for training.\n",
    "cnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"AUC\", \"accuracy\"])\n",
    "\n",
    "# Split the dataset into train and validation set for training and evaludating the model\n",
    "X_train, X_val, y_train, y_val = train_test_split(trainvalid_data, train_labels, shuffle=True, random_state=123)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "\n",
    "# Trains the model for a fixed number of epochs (iterations on a dataset)\n",
    "history = cnn_model.fit(X_train, y_train, batch_size=128, epochs=20, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d415dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      4829\n",
      "           1       0.83      0.53      0.65       974\n",
      "           2       0.88      0.95      0.91      1499\n",
      "           3       0.78      0.82      0.80      1056\n",
      "           4       0.79      0.94      0.86      1040\n",
      "           5       0.88      0.87      0.87       672\n",
      "\n",
      "    accuracy                           0.90     10070\n",
      "   macro avg       0.86      0.85      0.84     10070\n",
      "weighted avg       0.90      0.90      0.89     10070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the CNN model to output probabilities on test data\n",
    "y_preds = cnn_model.predict(test_data)\n",
    "\n",
    "y_vals = test_labels.to_numpy()\n",
    "# Model Probabilities for class 1 of each of the target variables\n",
    "# y_preds = np.transpose(np.array(cnn_model.predict_proba(X_val))[:, :, 1])\n",
    "\n",
    "y_preds_2 = np.argmax(y_preds, axis=1)\n",
    "y_test_2 = np.argmax(y_vals, axis=1)\n",
    "print(f\"The Classification report: \\n\")\n",
    "print(classification_report(y_preds_2, y_test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35efb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_auc(y_test, y_pred):\n",
    "    aucs = []\n",
    "    # Calculate the ROC-AUC for each of the target column\n",
    "    for col in range(y_test.shape[1]):\n",
    "        aucs.append(roc_auc_score(y_test[:,col],y_pred[:,col]))\n",
    "    return aucs\n",
    "\n",
    "# Calculate Mean of the ROC-AUC\n",
    "mean_auc = mean(calculate_roc_auc(y_vals, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce80ddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9863582874569641"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933a130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_gpu]",
   "language": "python",
   "name": "conda-env-python_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
